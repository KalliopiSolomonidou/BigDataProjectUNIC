{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a425435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c03346a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded and cleaned.\n",
      "Data integration completed.\n",
      "Script completed.\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "earthquake_data_path = 'earthquakesIceland.csv' \n",
    "eruption_data_path = 'Eruption_Results.csv'      \n",
    "\n",
    "earthquake_data = pd.read_csv(earthquake_data_path)\n",
    "eruption_data = pd.read_csv(eruption_data_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Setting the first row as the header for the eruption data\n",
    "eruption_data.columns = eruption_data.iloc[0]\n",
    "eruption_data = eruption_data[1:]\n",
    "\n",
    "# Convert 'Start Year' and 'End Year' in eruption data to numeric\n",
    "eruption_data['Start Year'] = pd.to_numeric(eruption_data['Start Year'], errors='coerce')\n",
    "eruption_data['End Year'] = pd.to_numeric(eruption_data['End Year'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in 'Start Year' or 'End Year'\n",
    "eruption_data.dropna(subset=['Start Year', 'End Year'], inplace=True)\n",
    "\n",
    "print(\"Datasets loaded and cleaned.\")\n",
    "\n",
    "# Define if the seismic activity is followed by an eruption\n",
    "earthquake_data['Year'] = pd.to_datetime(earthquake_data['time']).dt.year\n",
    "earthquake_data['During Eruption'] = 0\n",
    "\n",
    "for _, eruption_row in eruption_data.iterrows():\n",
    "    eruption_year_range = range(int(eruption_row['Start Year']), int(eruption_row['End Year']) + 1)\n",
    "    matched = earthquake_data['Year'].isin(eruption_year_range).sum()\n",
    "    if matched > 0:\n",
    "        earthquake_data.loc[earthquake_data['Year'].isin(eruption_year_range), 'During Eruption'] = 1\n",
    "\n",
    "print(\"Data integration completed.\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Script completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df63477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed.\n",
      "Confusion Matrix:\n",
      "[[ 20  60]\n",
      " [ 11 217]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.25      0.36        80\n",
      "           1       0.78      0.95      0.86       228\n",
      "\n",
      "    accuracy                           0.77       308\n",
      "   macro avg       0.71      0.60      0.61       308\n",
      "weighted avg       0.75      0.77      0.73       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if there are enough classes for model training\n",
    "if earthquake_data['During Eruption'].nunique() < 2:\n",
    "    print(\"Not enough classes for model training. Exiting.\")\n",
    "else:\n",
    "    # Feature Selection and Model Building\n",
    "    X = earthquake_data[['latitude', 'longitude', 'depth', 'mag']]\n",
    "    y = earthquake_data['During Eruption']\n",
    "\n",
    "    # Splitting the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Logistic Regression Model\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Training the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Model training completed.\")\n",
    "\n",
    "    # Making predictions\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Model Evaluation\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "984df731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     latitude  longitude  depth  mag\n",
      "227   64.6366   -17.7710   7.30  4.6\n",
      "971   64.5070   -17.6250  33.00  5.2\n",
      "497   64.4457   -17.6238   7.11  5.0\n",
      "919   66.2680   -17.1240  10.00  4.2\n",
      "177   66.2493   -18.6779  10.00  5.4\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c72739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7695\n",
      "Decision Tree Accuracy: 0.7857\n",
      "Random Forest Accuracy: 0.8214\n",
      "Gradient Boosting Accuracy: 0.8442\n",
      "SVM Accuracy: 0.7403\n",
      "XGBoost Accuracy: 0.8377\n",
      "[LightGBM] [Info] Number of positive: 516, number of negative: 200\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 589\n",
      "[LightGBM] [Info] Number of data points in the train set: 716, number of used features: 4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.720670 -> initscore=0.947789\n",
      "[LightGBM] [Info] Start training from score 0.947789\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Accuracy: 0.8344\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Splitting the dataset\n",
    "X = earthquake_data[['latitude', 'longitude', 'depth', 'mag']]\n",
    "y = earthquake_data['During Eruption']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"LightGBM\": LGBMClassifier()\n",
    "}\n",
    "\n",
    "# Training and evaluating models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adf87fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from GridSearchCV:\n",
      "{'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'subsample': 0.8}\n",
      "Best score: 0.8044606964124563\n",
      "\n",
      "Best parameters from RandomizedSearchCV:\n",
      "{'subsample': 0.8, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "Best score: 0.8016771562181358\n",
      "\n",
      "Classification report for the best model from GridSearchCV:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.59      0.66        80\n",
      "           1       0.87      0.93      0.90       228\n",
      "\n",
      "    accuracy                           0.84       308\n",
      "   macro avg       0.81      0.76      0.78       308\n",
      "weighted avg       0.83      0.84      0.83       308\n",
      "\n",
      "\n",
      "Classification report for the best model from RandomizedSearchCV:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.61      0.67        80\n",
      "           1       0.87      0.93      0.90       228\n",
      "\n",
      "    accuracy                           0.84       308\n",
      "   macro avg       0.81      0.77      0.78       308\n",
      "weighted avg       0.84      0.84      0.84       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Splitting the dataset\n",
    "X = earthquake_data[['latitude', 'longitude', 'depth', 'mag']]\n",
    "y = earthquake_data['During Eruption']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier()\n",
    "\n",
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters from GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(gb_model, param_grid, n_iter=10, cv=3, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters from RandomizedSearchCV:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"Best score:\", random_search.best_score_)\n",
    "\n",
    "# Evaluating the best model from GridSearchCV\n",
    "best_model_grid = grid_search.best_estimator_\n",
    "grid_predictions = best_model_grid.predict(X_test)\n",
    "print(\"\\nClassification report for the best model from GridSearchCV:\")\n",
    "print(classification_report(y_test, grid_predictions))\n",
    "\n",
    "# Evaluating the best model from RandomizedSearchCV\n",
    "best_model_random = random_search.best_estimator_\n",
    "random_predictions = best_model_random.predict(X_test)\n",
    "print(\"\\nClassification report for the best model from RandomizedSearchCV:\")\n",
    "print(classification_report(y_test, random_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516ee8a",
   "metadata": {},
   "source": [
    "#### I tested both GridSearchCV and RandomizedSearchCV to determine the best parameters. Because the daily seismic activity is not very high, I chose to use GridSearchCV, but I kept both in case the number of events increases, and GridSearchCV becomes computationally expensive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bb674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save the model to a file\n",
    "with open('best_gradient_boosting_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5eafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
